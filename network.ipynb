{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\"Louis, I think this is the beginning of a beautiful friendship.\"</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data\n",
    "print(\"Reading data...\")\n",
    "data_filenames = []\n",
    "for root, dirs, files in os.walk('data/raw_images/'):  \n",
    "    for filename in files:\n",
    "        data_filenames.append('data/raw_images/'+filename)\n",
    "\n",
    "# data = [np.array(Image.open('data/raw_images/' + filename)) for filename in data_filenames]\n",
    "# print(\"Number of raw images: \\t\", end=\"\")\n",
    "# print(len(data))\n",
    "\n",
    "print(\"\\nReading annotated images of segmentation...\")\n",
    "annot_filenames = []\n",
    "for root, dirs, files in os.walk('data/class_color/'):  \n",
    "    for filename in files:\n",
    "        annot_filenames.append(filename)\n",
    "    \n",
    "annot = [np.array(Image.open('data/class_color/' + filename).convert('RGB')) for filename in annot_filenames ]\n",
    "print(\"Number of annotated images: \\t\", end=\"\")\n",
    "print(len(annot))\n",
    "# if len(data)==len(annot):\n",
    "#     print(\"\\nAll raw images are annotated.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading .csv file containing metadata about the segmentation\n",
    "print(\"Reading file containing metadata about the segmentation...\")\n",
    "metadf = pd.read_csv('data/categories.csv', sep=',')\n",
    "\n",
    "#Organizing subcategories into an array, and counting subcategories\n",
    "subcat = []\n",
    "no_subcat = 0\n",
    "for row in metadf.name:\n",
    "    subcat.append(row)\n",
    "no_subcat = len(subcat)\n",
    "\n",
    "#Organizing categories into an array\n",
    "cat = []\n",
    "for row in metadf.category:\n",
    "    cat.append(row)\n",
    "\n",
    "#Organizing category Ids into an array\n",
    "catid = []\n",
    "for row in metadf.catId:\n",
    "    catid.append(row)\n",
    "#Counting categories\n",
    "no_cat = 1\n",
    "act = catid[0]\n",
    "categories = [] #array containing categories without duplication\n",
    "categories.append(cat[0])\n",
    "for i in range(len(catid)):\n",
    "    if catid[i]!=act:\n",
    "        categories.append(cat[i])\n",
    "        no_cat+=1\n",
    "        act=catid[i]\n",
    "\n",
    "#Organizing subcategory RGB colors into an array\n",
    "col = []\n",
    "for row in metadf.color:\n",
    "    c = row.replace(\" \", \"\").split(',')\n",
    "    rgb = []\n",
    "    for i in c:\n",
    "        rgb.append(int(i))\n",
    "    col.append(rgb)\n",
    "\n",
    "\n",
    "print('Number of segmentation subcategories:', no_subcat)\n",
    "print('Number of segmentation categories:', no_cat, \"\\n\")\n",
    "print(\"Subcategories and their representational colors [R, G, B]: \\n\")\n",
    "for i in range(len(subcat)):\n",
    "    print(\"%30s \\t\" % subcat[i], end =\"\")\n",
    "    print(col[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catid_annot_filenames = []\n",
    "for root, dirs, files in os.walk('data/catid_annot/'):  \n",
    "    for filename in files:\n",
    "        catid_annot_filenames.append('data/catid_annot/'+filename)\n",
    "\n",
    "catid_annot = []\n",
    "if len(catid_annot_filenames) == len(annot):\n",
    "    print('Catid Files Already Exist')\n",
    "#     for img in range(len(annot)):\n",
    "#         catid_annot_img = np.fromfile('data/catid_annot/'+str(img) +'.txt', dtype=int)\n",
    "#         catid_annot.append(catid_annot_img)\n",
    "        \n",
    "if len(catid_annot_filenames) != len(annot):\n",
    "    print('Catid Files DO NOT Exist')\n",
    "    for img in range(len(annot)):\n",
    "        catid_annot_img = []\n",
    "        for index, row in enumerate(annot[img]):\n",
    "            blabla = []\n",
    "            for j, pixel in enumerate(row):\n",
    "                blabla.append(col.index(list(row[j])))\n",
    "            catid_annot_img.append(blabla)\n",
    "        np.array(catid_annot_img).tofile('data/catid_annot/' +str(img) +'.txt')\n",
    "        catid_annot.append(catid_annot_img)\n",
    "        catid_annot_filenames = []\n",
    "        for root, dirs, files in os.walk('data/catid_annot/'):  \n",
    "            for filename in files:\n",
    "                catid_annot_filenames.append('data/catid_annot/'+filename)\n",
    "        \n",
    "# if len(catid_annot) == len(annot):\n",
    "#     print('Reading in subcategory id annotated matrices: successful')\n",
    "# else :\n",
    "#     print('Reading in subcategory id annotated matrices: UNSUCCESSFUL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train-validation-test parts with ratios 70-20-10\n",
    "print(\"Splitting data into training data, validation data, test data\")\n",
    "nb_samples=len(data_filenames)\n",
    "#Splitting ratios:\n",
    "valid_split = 0.2\n",
    "test_split = 0.1\n",
    "train_split = 0.7\n",
    "print(\"The ratios are: \")\n",
    "print(\"\\t train:\\t\", train_split )\n",
    "print(\"\\t validation:\\t\",valid_split )\n",
    "print(\"\\t test:\\t\",test_split)\n",
    "    \n",
    "#Splitting\n",
    "data_train = np.array(data_filenames[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "annot_train = np.array(catid_annot_filenames[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "data_valid = data_filenames[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "annot_valid = catid_annot_filenames[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "data_test  = data_filenames[int(nb_samples*(1-test_split)):]\n",
    "annot_test  = catid_annot_filenames[int(nb_samples*(1-test_split)):]\n",
    "\n",
    "# #Separation of axes (RGB channels)\n",
    "# red_train = []\n",
    "# green_train = []\n",
    "# blue_train = []\n",
    "# for img in data_train:\n",
    "#     image = np.array(img.ravel(), dtype='float64')\n",
    "#     red_train.append(image[0::3])\n",
    "#     green_train.append(image[1::3])\n",
    "#     blue_train.append(image[2::3])\n",
    "\n",
    "\n",
    "# #Standardizing\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# scaler.fit(np.reshape(red_train, (-1, 1)))\n",
    "# red_std = scaler.transform(red_train)\n",
    "\n",
    "# scaler.fit(np.reshape(green_train, (-1,1)))\n",
    "# green_std = scaler.transform(green_train)\n",
    "\n",
    "# scaler.fit(np.reshape(blue_train, (-1,1)))\n",
    "# blue_std = scaler.transform(blue_train)\n",
    "\n",
    "print(\"\\nNumber of training samples:\\t\", len(data_train))\n",
    "print(\"Number of validation samples:\\t\", len(data_valid))\n",
    "print(\"Number of test samples:\\t\", len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications import imagenet_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "# Device check\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    return imagenet_utils.preprocess_input(x, mode='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, labels, batch_size=32, dim=(1024, 2048), n_classes=34, shuffle=True):\n",
    "    # Initialization\n",
    "    data_size = len(images)\n",
    "    nbatches = data_size // batch_size\n",
    "    list_IDs = np.arange(data_size)\n",
    "    indices = list_IDs\n",
    "    \n",
    "    # Data generation\n",
    "    while True:\n",
    "        if shuffle == True:\n",
    "            np.random.shuffle(indices)\n",
    "        for index in range(nbatches):\n",
    "            batch_indices = indices[index*batch_size:(index+1)*batch_size]\n",
    "\n",
    "            X = np.empty((batch_size, *dim, 3))\n",
    "            y_semseg = np.empty((batch_size, *dim), dtype=int)\n",
    "\n",
    "            for i, ID in enumerate(batch_indices):\n",
    "                image = cv2.resize(np.array(imageio.imread(images[ID]), dtype=np.uint8), dim[1::-1])\n",
    "                catid_annot_img = np.fromfile(labels[ID], dtype=int)\n",
    "                label = cv2.resize(catid_annot_img, dim[1::-1], interpolation=cv2.INTER_NEAREST)\n",
    "                X[i,] = image\n",
    "\n",
    "#                 fujj = []\n",
    "                \n",
    "#                 for index, row in enumerate(label):\n",
    "#                     blabla = []\n",
    "#                     for j, pixel in enumerate(row):\n",
    "#                         blabla.append(col.index(list(row[j])))\n",
    "#                     fujj.append(blabla)\n",
    "#                    #     print(pixel)\n",
    "#                #     print(col.index(row[index]))\n",
    "#                 #        print(label[n])\n",
    "                \n",
    "                    \n",
    "#          #       print(fujj.shape)\n",
    "                y_semseg[i] = label\n",
    "   \n",
    "            yield (preprocess_input(X), to_categorical(y_semseg, num_classes=n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "data_shape= imageio.imread(data_train[0]).shape[:2]\n",
    "data_shape= (int(data_shape[0]/2), int(data_shape[1]/2))\n",
    "classes = no_subcat\n",
    "\n",
    "print(data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(data_train[:10], annot_train[:10], batch_size=batch_size, dim=data_shape, n_classes=classes)\n",
    "val_generator = data_generator(data_valid[:10], annot_valid[:10], batch_size=batch_size, dim=data_shape, n_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image, label = next(train_generator)\n",
    "image = image[i]\n",
    "label = np.argmax(label[i], axis=-1)\n",
    "\n",
    "fig=plt.figure(figsize=(20, 10))\n",
    "\n",
    "cm = plt.get_cmap('gist_ncar')\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(image * .5 + .5)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow((image * .5 + .5) * .6 + cm(label/34.)[...,:3] * .4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image, label = next(train_generator)\n",
    "image = image[i]\n",
    "label = np.argmax(label[i], axis=-1)\n",
    "\n",
    "fig=plt.figure(figsize=(20, 10))\n",
    "\n",
    "cm = plt.get_cmap('gist_ncar')\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(image * .5 + .5)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow((image * .5 + .5) * .6 + cm(label/34.)[...,:3] * .4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.models as models\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Permute\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding_layers(input_layer):\n",
    "    kernel = 3\n",
    "    filter_size = 64\n",
    "    pool_size = 2\n",
    "    \n",
    "    x = Conv2D(filter_size, kernel, padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "\n",
    "    x = Conv2D(128, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "\n",
    "    x = Conv2D(256, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "\n",
    "    x = Conv2D(512, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoding_layers(input_layer):\n",
    "    kernel = 3\n",
    "    filter_size = 64\n",
    "    pool_size = 2\n",
    "\n",
    "    x = Conv2D(512, kernel, padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(pool_size,pool_size))(x)\n",
    "    x = Conv2D(256, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(pool_size,pool_size))(x)\n",
    "    x = Conv2D(128, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(pool_size,pool_size))(x)\n",
    "    x = Conv2D(filter_size, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input((*data_shape, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layer = create_encoding_layers(input_layer)\n",
    "decoded_layer = create_decoding_layers(encoded_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = Conv2D(classes, 1, padding='same')(decoded_layer)\n",
    "final_layer = Activation('softmax')(final_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_model = Model(inputs=input_layer, outputs=final_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_model.fit_generator(generator=train_generator,\n",
    "                          steps_per_epoch=len(data_train) // batch_size,\n",
    "                           epochs=1, validation_data=val_generator,\n",
    "                           validation_steps=len(data_valid) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_model.save_weights('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
