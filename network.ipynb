{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\"Louis, I think this is the beginning of a beautiful friendship.\"</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of raw images: \t100\n",
      "\n",
      "Reading annotated images of segmentation...\n",
      "Number of annotated images: \t100\n",
      "\n",
      "All raw images are annotated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading data\n",
    "print(\"Reading data...\")\n",
    "data_filenames = []\n",
    "for root, dirs, files in os.walk('data/raw_images/'):  \n",
    "    for filename in files:\n",
    "        data_filenames.append(filename)\n",
    "\n",
    "data = [np.array(Image.open('data/raw_images/' + filename)) for filename in data_filenames]\n",
    "print(\"Number of raw images: \\t\", end=\"\")\n",
    "print(len(data))\n",
    "\n",
    "print(\"\\nReading annotated images of segmentation...\")\n",
    "annot_filenames = []\n",
    "for root, dirs, files in os.walk('data/class_color/'):  \n",
    "    for filename in files:\n",
    "        annot_filenames.append(filename)\n",
    "        \n",
    "annot = [np.array(Image.open('data/class_color/' + filename)) for filename in annot_filenames ]\n",
    "print(\"Number of annotated images: \\t\", end=\"\")\n",
    "print(len(annot))\n",
    "if len(data)==len(annot):\n",
    "    print(\"\\nAll raw images are annotated.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training data, validation data, test data\n",
      "The ratios are: \n",
      "\t train:\t 0.7\n",
      "\t validation::\t 0.2\n",
      "\t test:\t 0.1\n",
      "\n",
      "Number of training samples:\t 70\n",
      "Number of validation samples:\t 20\n",
      "Number of test samples:\t 10\n"
     ]
    }
   ],
   "source": [
    "#Splitting data into train-validation-test parts with ratios 70-20-10\n",
    "print(\"Splitting data into training data, validation data, test data\")\n",
    "nb_samples=len(data_filenames)\n",
    "#Splitting ratios:\n",
    "valid_split = 0.2\n",
    "test_split = 0.1\n",
    "train_split = 0.7\n",
    "print(\"The ratios are: \")\n",
    "print(\"\\t train:\\t\", train_split )\n",
    "print(\"\\t validation::\\t\",valid_split )\n",
    "print(\"\\t test:\\t\",test_split)\n",
    "    \n",
    "#Splitting\n",
    "data_train = np.array(data[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "annot_train = np.array(annot[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "data_valid = data[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "annot_valid = annot[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "data_test  = data[int(nb_samples*(1-test_split)):]\n",
    "annot_test  = annot[int(nb_samples*(1-test_split)):]\n",
    "\n",
    "#Separation of axes (RGB channels)\n",
    "red_train = []\n",
    "green_train = []\n",
    "blue_train = []\n",
    "for img in data_train:\n",
    "    image = np.array(img.ravel(), dtype='float64')\n",
    "    red_train.append(image[0::3])\n",
    "    green_train.append(image[1::3])\n",
    "    blue_train.append(image[2::3])\n",
    "\n",
    "\n",
    "#Standardizing\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(np.reshape(red_train, (-1, 1)))\n",
    "red_std = scaler.transform(red_train)\n",
    "\n",
    "scaler.fit(np.reshape(green_train, (-1,1)))\n",
    "green_std = scaler.transform(green_train)\n",
    "\n",
    "scaler.fit(np.reshape(blue_train, (-1,1)))\n",
    "blue_std = scaler.transform(blue_train)\n",
    "\n",
    "print(\"\\nNumber of training samples:\\t\", len(data_train))\n",
    "print(\"Number of validation samples:\\t\", len(data_valid))\n",
    "print(\"Number of test samples:\\t\", len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file containing metadata about the segmentation...\n",
      "Number of segmentation subcategories: 41\n",
      "Number of segmentation categories: 8 \n",
      "\n",
      "Subcategories and their representational colors [R, G, B]: \n",
      "\n",
      "                     unlabeled \t[0, 0, 0]\n",
      "                       dynamic \t[111, 74, 0]\n",
      "                   ego vehicle \t[0, 0, 0]\n",
      "                        ground \t[81, 0, 81]\n",
      "                        static \t[0, 0, 0]\n",
      "                       parking \t[250, 170, 160]\n",
      "                    rail track \t[230, 150, 140]\n",
      "                          road \t[128, 64, 128]\n",
      "                      sidewalk \t[244, 35, 232]\n",
      "                        bridge \t[150, 100, 100]\n",
      "                      building \t[70, 70, 70]\n",
      "                         fence \t[190, 153, 153]\n",
      "                        garage \t[180, 100, 180]\n",
      "                    guard rail \t[180, 165, 180]\n",
      "                        tunnel \t[150, 120, 90]\n",
      "                         wall  \t[102, 102, 156]\n",
      "                        banner \t[250, 170, 100]\n",
      "                     billboard \t[220, 220, 250]\n",
      "                  lane divider \t[255, 165, 0]\n",
      "                  parking sign \t[220, 20, 60]\n",
      "                          pole \t[153, 153, 153]\n",
      "                     polegroup \t[153, 153, 153]\n",
      "                  street light \t[220, 220, 100]\n",
      "                  traffic cone \t[255, 70, 0]\n",
      "                traffic device \t[220, 220, 220]\n",
      "                 traffic light \t[250, 170, 30]\n",
      "                  traffic sign \t[220, 220, 0]\n",
      "            traffic sign frame \t[250, 170, 250]\n",
      "                       terrain \t[152, 251, 152]\n",
      "                    vegetation \t[107, 142, 35]\n",
      "                           sky \t[70, 130, 180]\n",
      "                        person \t[220, 20, 60]\n",
      "                         rider \t[255, 0, 0]\n",
      "                       bicycle \t[119, 11, 32]\n",
      "                           bus \t[0, 60, 100]\n",
      "                           car \t[0, 0, 142]\n",
      "                       caravan \t[0, 0, 90]\n",
      "                    motorcycle \t[0, 0, 230]\n",
      "                       trailer \t[0, 0, 110]\n",
      "                         train \t[0, 80, 100]\n",
      "                         truck \t[0, 0, 70]\n"
     ]
    }
   ],
   "source": [
    "#Reading .csv file containing metadata about the segmentation\n",
    "print(\"Reading file containing metadata about the segmentation...\")\n",
    "metadf = pd.read_csv('data/categories.csv', sep=',')\n",
    "\n",
    "#Organizing subcategories into an array, and counting subcategories\n",
    "subcat = []\n",
    "no_subcat = 0\n",
    "for row in metadf.name:\n",
    "    subcat.append(row)\n",
    "no_subcat = len(subcat)\n",
    "\n",
    "#Organizing categories into an array\n",
    "cat = []\n",
    "for row in metadf.category:\n",
    "    cat.append(row)\n",
    "\n",
    "#Organizing category Ids into an array\n",
    "catid = []\n",
    "for row in metadf.catId:\n",
    "    catid.append(row)\n",
    "#Counting categories\n",
    "no_cat = 1\n",
    "act = catid[0]\n",
    "categories = [] #array containing categories without duplication\n",
    "categories.append(cat[0])\n",
    "for i in range(len(catid)):\n",
    "    if catid[i]!=act:\n",
    "        categories.append(cat[i])\n",
    "        no_cat+=1\n",
    "        act=catid[i]\n",
    "\n",
    "#Organizing subcategory RGB colors into an array\n",
    "col = []\n",
    "for row in metadf.color:\n",
    "    c = row.replace(\" \", \"\").split(',')\n",
    "    rgb = []\n",
    "    for i in c:\n",
    "        rgb.append(int(i))\n",
    "    col.append(rgb)\n",
    "\n",
    "\n",
    "print('Number of segmentation subcategories:', no_subcat)\n",
    "print('Number of segmentation categories:', no_cat, \"\\n\")\n",
    "print(\"Subcategories and their representational colors [R, G, B]: \\n\")\n",
    "for i in range(len(subcat)):\n",
    "    print(\"%30s \\t\" % subcat[i], end =\"\")\n",
    "    print(col[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications import imagenet_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6224024927722524262\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "# Device check\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    return imagenet_utils.preprocess_input(x, mode='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(images, labels, batch_size=32, dim=(1024, 2048), n_classes=34, shuffle=True):\n",
    "    # Initialization\n",
    "    data_size = len(images)\n",
    "    nbatches = data_size // batch_size\n",
    "    list_IDs = np.arange(data_size)\n",
    "    indices = list_IDs\n",
    "    \n",
    "    # Data generation\n",
    "    while True:\n",
    "        if shuffle == True:\n",
    "            np.random.shuffle(indices)\n",
    "        for index in range(nbatches):\n",
    "            batch_indices = indices[index*batch_size:(index+1)*batch_size]\n",
    "\n",
    "            X = np.empty((batch_size, *dim, 3))\n",
    "            y_semseg = np.empty((batch_size, *dim), dtype=int)\n",
    "\n",
    "            for i, ID in enumerate(batch_indices):\n",
    "                image = cv2.resize(np.array(imageio.imread(images[ID]), dtype=np.uint8), dim[1::-1])\n",
    "                label = cv2.resize(imageio.imread(labels[ID]), dim[1::-1], interpolation=cv2.INTER_NEAREST)\n",
    "                X[i,] = image\n",
    "                y_semseg[i] = label\n",
    "            yield (preprocess_input(X), to_categorical(y_semseg, num_classes=n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "data_shape= data[0].shape[:2]\n",
    "classes = no_subcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(data_train, annot_train, batch_size=batch_size, dim=data_shape, n_classes=classes)\n",
    "val_generator = data_generator(val_images, val_labels, batch_size=batch_size, dim=data_shape, n_classes=classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
