{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\"Louis, I think this is the beginning of a beautiful friendship.\"</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Number of raw images: \t100\n",
      "\n",
      "Reading annotated images of segmentation...\n",
      "Number of annotated images: \t100\n",
      "\n",
      "All raw images are annotated.\n",
      "\n",
      "Splitting data into training data, validation data, test data\n",
      "The ratios are: \n",
      "\t train:\t 0.7\n",
      "\t validation::\t 0.2\n",
      "\t test:\t 0.1\n",
      "\n",
      "Standardized data:\n",
      "Red:\n"
     ]
    }
   ],
   "source": [
    "#Reading data\n",
    "print(\"Reading data...\")\n",
    "data_filenames = []\n",
    "for root, dirs, files in os.walk('data/raw_images/'):  \n",
    "    for filename in files:\n",
    "        data_filenames.append(filename)\n",
    "\n",
    "data = [np.array(Image.open('data/raw_images/' + filename)) for filename in data_filenames]\n",
    "print(\"Number of raw images: \\t\", end=\"\")\n",
    "print(len(data))\n",
    "\n",
    "print(\"\\nReading annotated images of segmentation...\")\n",
    "annot_filenames = []\n",
    "for root, dirs, files in os.walk('data/class_color/'):  \n",
    "    for filename in files:\n",
    "        annot_filenames.append(filename)\n",
    "        \n",
    "annot = [np.array(Image.open('data/class_color/' + filename)) for filename in annot_filenames ]\n",
    "print(\"Number of annotated images: \\t\", end=\"\")\n",
    "print(len(annot))\n",
    "if len(data)==len(annot):\n",
    "    print(\"\\nAll raw images are annotated.\\n\")\n",
    "\n",
    "#Splitting data into train-validation-test parts with ratios 70-20-10\n",
    "print(\"Splitting data into training data, validation data, test data\")\n",
    "nb_samples=len(data_filenames)\n",
    "#Splitting ratios:\n",
    "valid_split = 0.2\n",
    "test_split = 0.1\n",
    "train_split = 0.7\n",
    "print(\"The ratios are: \")\n",
    "print(\"\\t train:\\t\", train_split )\n",
    "print(\"\\t validation::\\t\",valid_split )\n",
    "print(\"\\t test:\\t\",test_split)\n",
    "    \n",
    "#Splitting\n",
    "data_train = np.array(data[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "annot_train = np.array(annot[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "data_valid = data[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "annot_valid = annot[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "data_test  = data[int(nb_samples*(1-test_split)):]\n",
    "annot_test  = annot[int(nb_samples*(1-test_split)):]\n",
    "\n",
    "#Separation of axes (RGB channels)\n",
    "red_train = []\n",
    "green_train = []\n",
    "blue_train = []\n",
    "for img in data_train:\n",
    "    image = np.array(img.ravel(), dtype='float64')\n",
    "    red_train.append(image[0::3])\n",
    "    green_train.append(image[1::3])\n",
    "    blue_train.append(image[2::3])\n",
    "\n",
    "\n",
    "#Standardizing\n",
    "scaler = StandardScaler()\n",
    "    \n",
    "print(\"\\nStandardized data:\\nRed:\")\n",
    "scaler.fit(np.reshape(red_train, (-1, 1)))\n",
    "red_std = scaler.transform(red_train)\n",
    "print(red_std)\n",
    "\n",
    "print(\"\\nGreen:\")\n",
    "scaler.fit(np.reshape(green_train, (-1,1)))\n",
    "green_std = scaler.transform(green_train)\n",
    "print(green_std)\n",
    "    \n",
    "print(\"\\nBlue:\")\n",
    "scaler.fit(np.reshape(blue_train, (-1,1)))\n",
    "blue_std = scaler.transform(blue_train)\n",
    "print(blue_std)\n",
    "\n",
    "print(\"\\nNumber of training samples:\\t\", len(data_train))\n",
    "print(\"Number of validation samples:\\t\", len(data_valid))\n",
    "print(\"Number of test samples:\\t\", len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7281753732200905582\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import json\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "# Device check\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
