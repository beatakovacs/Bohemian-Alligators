{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\"Louis, I think this is the beginning of a beautiful friendship.\"<br><br>Choosing the right architecture</h1>\n",
    "\n",
    "<h4>AWS Adventures</h4>\n",
    "Our original intention was to first explore Andr√°s Kohlmann's SegNet based Segnet-- FCN architecture from the October 9th lecture, and then proceed with playing around with it, building it further, mixing it up. Unfortunately things didn't go as planned, because both our laptops ran out of memory when wanting to train the network, which was a surprise, because Bea has a great one equiped with a strong Nvidia GPU. \n",
    "That's why we turned to Amason Web Services, which took ages to configure properly.\n",
    "\n",
    "The host is finally set up. (Kind of... still having some problems with it sometimes.)\n",
    "\n",
    "<h4>SegNet in a Nutshell</h4>\n",
    "SegNet is a Fully Convolutional Network, and therefore follows the encoder-decoder architecture. Ditching the fully connected layers is necessary because for segmentation, the spatial location of the pixels is obviously important and convolutional layers opposed to fully connected layers handle that perfectly, simply because of the way they work. That's the main reason why SegNet is a great architecture for semantic pixel-level image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preparing the Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file containing metadata about the segmentation...\n",
      "Number of segmentation subcategories: 41\n",
      "Number of segmentation categories: 8 \n",
      "\n",
      "Subcategories and their representational colors [R, G, B]: \n",
      "\n",
      "                     unlabeled \t0    [0, 0, 0]\n",
      "                       dynamic \t1    [111, 74, 0]\n",
      "                   ego vehicle \t2    [0, 0, 0]\n",
      "                        ground \t3    [81, 0, 81]\n",
      "                        static \t4    [0, 0, 0]\n",
      "                       parking \t5    [250, 170, 160]\n",
      "                    rail track \t6    [230, 150, 140]\n",
      "                          road \t7    [128, 64, 128]\n",
      "                      sidewalk \t8    [244, 35, 232]\n",
      "                        bridge \t9    [150, 100, 100]\n",
      "                      building \t10    [70, 70, 70]\n",
      "                         fence \t11    [190, 153, 153]\n",
      "                        garage \t12    [180, 100, 180]\n",
      "                    guard rail \t13    [180, 165, 180]\n",
      "                        tunnel \t14    [150, 120, 90]\n",
      "                         wall  \t15    [102, 102, 156]\n",
      "                        banner \t16    [250, 170, 100]\n",
      "                     billboard \t17    [220, 220, 250]\n",
      "                  lane divider \t18    [255, 165, 0]\n",
      "                  parking sign \t19    [220, 20, 60]\n",
      "                          pole \t20    [153, 153, 153]\n",
      "                     polegroup \t21    [153, 153, 153]\n",
      "                  street light \t22    [220, 220, 100]\n",
      "                  traffic cone \t23    [255, 70, 0]\n",
      "                traffic device \t24    [220, 220, 220]\n",
      "                 traffic light \t25    [250, 170, 30]\n",
      "                  traffic sign \t26    [220, 220, 0]\n",
      "            traffic sign frame \t27    [250, 170, 250]\n",
      "                       terrain \t28    [152, 251, 152]\n",
      "                    vegetation \t29    [107, 142, 35]\n",
      "                           sky \t30    [70, 130, 180]\n",
      "                        person \t31    [220, 20, 60]\n",
      "                         rider \t32    [255, 0, 0]\n",
      "                       bicycle \t33    [119, 11, 32]\n",
      "                           bus \t34    [0, 60, 100]\n",
      "                           car \t35    [0, 0, 142]\n",
      "                       caravan \t36    [0, 0, 90]\n",
      "                    motorcycle \t37    [0, 0, 230]\n",
      "                       trailer \t38    [0, 0, 110]\n",
      "                         train \t39    [0, 80, 100]\n",
      "                         truck \t40    [0, 0, 70]\n"
     ]
    }
   ],
   "source": [
    "#Reading .csv file containing metadata about the segmentation\n",
    "print(\"Reading file containing metadata about the segmentation...\")\n",
    "metadf = pd.read_csv('data/categories.csv', sep=',')\n",
    "\n",
    "#Organizing subcategories into an array, and counting subcategories\n",
    "subcat = []\n",
    "no_subcat = 0\n",
    "for row in metadf.name:\n",
    "    subcat.append(row)\n",
    "no_subcat = len(subcat)\n",
    "\n",
    "#Organizing categories into an array\n",
    "cat = []\n",
    "for row in metadf.category:\n",
    "    cat.append(row)\n",
    "\n",
    "#Organizing category Ids into an array\n",
    "catid = []\n",
    "for row in metadf.catId:\n",
    "    catid.append(row)\n",
    "#Counting categories\n",
    "no_cat = 1\n",
    "act = catid[0]\n",
    "categories = [] #array containing categories without duplication\n",
    "categories.append(cat[0])\n",
    "for i in range(len(catid)):\n",
    "    if catid[i]!=act:\n",
    "        categories.append(cat[i])\n",
    "        no_cat+=1\n",
    "        act=catid[i]\n",
    "\n",
    "#Organizing subcategory RGB colors into an array\n",
    "col = []\n",
    "for row in metadf.color:\n",
    "    c = row.replace(\" \", \"\").split(',')\n",
    "    rgb = []\n",
    "    for i in c:\n",
    "        rgb.append(int(i))\n",
    "    col.append(rgb)\n",
    "\n",
    "\n",
    "print('Number of segmentation subcategories:', no_subcat)\n",
    "print('Number of segmentation categories:', no_cat, \"\\n\")\n",
    "print(\"Subcategories and their IDs and the representational colors [R, G, B]: \\n\")\n",
    "for i in range(len(subcat)):\n",
    "    print(\"%30s \\t\" % subcat[i], end =\"\")\n",
    "    print(i, \"  \", col[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading filenames\n",
    "data_filenames = []\n",
    "for root, dirs, files in os.walk('data/raw_images/'):  \n",
    "    for filename in files:\n",
    "        data_filenames.append(filename)\n",
    "\n",
    "annot_filenames = []\n",
    "for root, dirs, files in os.walk('data/class_color/'):  \n",
    "    for filename in files:\n",
    "        annot_filenames.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Serializing Annotation Matrices</h1>\n",
    "Our dataset originally contained pixel-level annotations in the form of .png files. We choose to train our model with the corresponding annotation-matrices, built so that each pixel has the id of the class annotated to them.\n",
    "\n",
    "The annotation are serialized on the same filename, as the corresponding raw data images for easier use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for files in the corresponding folder\n",
    "catid_annot_filenames = []\n",
    "for root, dirs, files in os.walk('data/catid_annot/'):  \n",
    "    for filename in files:\n",
    "        catid_annot_filenames.append('data/catid_annot/'+filename)\n",
    "\n",
    "#if all the annotationfiles exist, there's no need to create them\n",
    "if len(catid_annot_filenames) == len(annot_filenames):\n",
    "    print('Subcategory-Id-Annotation Files Already Exist')\n",
    "\n",
    "#if not, then the .png annotation files should be loaded, \n",
    "#the matrices should be created, and they should be serialized\n",
    "if len(catid_annot_filenames) != len(annot_filenames):\n",
    "    print('Subcategory-Id-Annotation Files DO NOT Exist')\n",
    "    \n",
    "    for image in range(len(annot_filenames)): #iterationg over annotation-image filenames\n",
    "        filename = annot_filenames[image]\n",
    "        #loading .png image, converting it to have RGB channels only\n",
    "        img = np.array(Image.open('data/class_color/' + filename).convert('RGB'))\n",
    "        catid_annot_img = [] #this is gonna be our new matrice\n",
    "        for i, row in enumerate(img): #iterating over rows\n",
    "            catid_row = []\n",
    "            for j, pixel in enumerate(row): #iterating over pixels\n",
    "                catid_row.append(col.index(list(row[j]))) #appending the corresponding subcategory id\n",
    "            catid_annot_img.append(catid_row) \n",
    "         \n",
    "        #saving the matrices\n",
    "        np.array(catid_annot_img).tofile('data/catid_annot/' + data_filenames[image])\n",
    "    \n",
    "    #double checking if all the matrices have been serialized    \n",
    "    for root, dirs, files in os.walk('data/catid_annot/'):  \n",
    "        for filename in files:\n",
    "            catid_annot_filenames.append('data/catid_annot/'+filename)\n",
    "    if len(catid_annot_filenames) == len(annot_filenames):\n",
    "        print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train-validation-test parts with ratios 70-20-10\n",
    "print(\"Splitting data into training data, validation data, test data\")\n",
    "nb_samples=len(data_filenames)\n",
    "#Splitting ratios:\n",
    "valid_split = 0.2\n",
    "test_split = 0.1\n",
    "train_split = 0.7\n",
    "print(\"The ratios are: \")\n",
    "print(\"\\t train:\\t\", train_split )\n",
    "print(\"\\t validation:\\t\",valid_split )\n",
    "print(\"\\t test:\\t\",test_split)\n",
    "    \n",
    "#Splitting\n",
    "#The serialized annotation files are on the same name but in a different directory,\n",
    "#so we only need to split one of the arrays.\n",
    "data_train = np.array(data_filenames[0:int(nb_samples*(1-valid_split-test_split))])\n",
    "data_valid = data_filenames[int(nb_samples*(1-valid_split-test_split)):int(nb_samples*(1-test_split))]\n",
    "data_test  = data_filenames[int(nb_samples*(1-test_split)):]\n",
    "\n",
    "print(\"\\nNumber of training samples:\\t\", len(data_train))\n",
    "print(\"Number of validation samples:\\t\", len(data_valid))\n",
    "print(\"Number of test samples:\\t\", len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications import imagenet_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "# Device check\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    return imagenet_utils.preprocess_input(x, mode='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Generator</h1>\n",
    "Using a Data Generator is great when working with huge datasets, because the training process is less memory consuming this way, as it generates your data on the fly.\n",
    "We definitely plan on keeping this approach later on as well.\n",
    "The data generator is also responsible for shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(filenames, batch_size=32, dim=(720, 1280), n_classes=41, shuffle=True):\n",
    "    # Initialization\n",
    "    data_size = len(filenames)\n",
    "    nbatches = data_size // batch_size\n",
    "    list_IDs = np.arange(data_size)\n",
    "    indices = list_IDs\n",
    "    \n",
    "    # Data generation\n",
    "    while True:\n",
    "        if shuffle == True:\n",
    "            np.random.shuffle(indices) #shuffling when Shuffle parameter is True\n",
    "            \n",
    "        for index in range(nbatches):\n",
    "            batch_indices = indices[index*batch_size:(index+1)*batch_size]\n",
    "\n",
    "            X = np.empty((batch_size, *dim, 3))\n",
    "            y_semseg = np.empty((batch_size, *dim), dtype=int)\n",
    "\n",
    "            for i, ID in enumerate(batch_indices):\n",
    "                #reading in the raw image on the fly\n",
    "                image = cv2.resize(np.array(imageio.imread('data/raw_images/' + filenames[ID]), dtype=np.uint8), dim[1::-1])\n",
    "                #loading in the serialized annotation file on the fly\n",
    "                catid_annot_img = np.fromfile('data/catid_annot/'+ filenames[ID], dtype=int)\n",
    "                catid_annot_img = np.reshape(catid_annot_img, (720, 1280))\n",
    "                label = cv2.resize(catid_annot_img, dim[1::-1], interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                X[i,] = image\n",
    "                y_semseg[i] = label\n",
    "   \n",
    "            yield (preprocess_input(X), to_categorical(y_semseg, num_classes=n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for the data generator\n",
    "batch_size = 4\n",
    "data_shape= imageio.imread('data/raw_images/' + data_train[0]).shape[:2]\n",
    "data_shape= (int(data_shape[0]/2), int(data_shape[1]/2))\n",
    "classes = no_subcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data generator for training and validating\n",
    "train_generator = data_generator(data_train, batch_size=batch_size, dim=data_shape, n_classes=classes)\n",
    "val_generator = data_generator(data_valid, batch_size=batch_size, dim=data_shape, n_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the train generator\n",
    "print(\"An example of the training data generated by the training data generator:\")\n",
    "i = 0\n",
    "image, label = next(train_generator)\n",
    "image = image[i]\n",
    "label = np.argmax(label[i], axis=-1)\n",
    "\n",
    "fig=plt.figure(figsize=(20, 10))\n",
    "\n",
    "cm = plt.get_cmap('gist_ncar')\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(image * .5 + .5)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow((image * .5 + .5) * .6 + cm(label/34.)[...,:3] * .4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.models as models\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape, Permute\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Defining the Encoder And Decoder </h1>\n",
    "This architecture has 4 encoders and 4 decoders. The decoders have max-pooling subsampling layers, and the encoders have their corresponding upsampling layers.\n",
    "\n",
    "Later on we definitely plan on implementing the connections between the decoders and the encoders, so that the encoders receive the max-pooling indices as their input for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding_layers(input_layer):\n",
    "    kernel = 3\n",
    "    filter_size = 64\n",
    "    pool_size = 2\n",
    "    \n",
    "    x = Conv2D(filter_size, kernel, padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "\n",
    "    x = Conv2D(128, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "\n",
    "    x = Conv2D(256, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "\n",
    "    x = Conv2D(512, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoding_layers(input_layer):\n",
    "    kernel = 3\n",
    "    filter_size = 64\n",
    "    pool_size = 2\n",
    "\n",
    "    x = Conv2D(512, kernel, padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(pool_size,pool_size))(x)\n",
    "    x = Conv2D(256, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(pool_size,pool_size))(x)\n",
    "    x = Conv2D(128, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(pool_size,pool_size))(x)\n",
    "    x = Conv2D(filter_size, kernel, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the layers\n",
    "input_layer = Input((*data_shape, 3))\n",
    "encoded_layer = create_encoding_layers(input_layer)\n",
    "decoded_layer = create_decoding_layers(encoded_layer)\n",
    "final_layer = Conv2D(classes, 1, padding='same')(decoded_layer)\n",
    "final_layer = Activation('softmax')(final_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "semseg_model = Model(inputs=input_layer, outputs=final_layer)\n",
    "#Compiling the model\n",
    "semseg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_model.fit_generator(generator=train_generator,\n",
    "                          steps_per_epoch=len(data_train) // batch_size,\n",
    "                           epochs=1, validation_data=val_generator,\n",
    "                           validation_steps=len(data_valid) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semseg_model.save_weights('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
